{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP24112 Summative Exercise (30 Marks)\n",
    "# On Linear Classification and MLP Regression\n",
    "\n",
    "In this lab exercise, you will build linear classifiers by gradient descent for loan approval classification (14 marks) and build MLP regressors for crop production prediction (16 marks), using two given datasets. To prepare for this lab exercise, you will\n",
    "* Get familiar with lecture content of Chapters 3-7.\n",
    "* Get familiar with how to build a regression model by mutlilayer perceptron (MLP) using the scikit learn tutorial (https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression).\n",
    "* Get familiar with basic scikit-learn tools for [data splitting](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html), used for setting machine learning experiments.\n",
    "\n",
    "\n",
    "You will submit a notebook file, a pdf report, and a trained model. You will be marked for implementation, design, result and analysis. Your code should be easy to read and your report should be concise (max 600 words). It is strongly recommended that you use a LaTeX editor, such as [Overleaf](https://www.overleaf.com/), to write your report. Handwritten reports will not be accepted.\n",
    "\n",
    "Please note your notebook should take no more than 10 minutes to run on a Google Colab instance. **Marks may be dropped for inefficient and unreadable code.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Loan Classification (14 marks)\n",
    "### 1.1 Dataset and Experiment Preparation\n",
    "\n",
    "**On Dataset**: The provided \"Loan Approval Classification Dataset\" contains financial and demographic information related to loan applications. It includes 45,000 instances with 14 features, covering applicant demographics, credit history, and loan details, with a mix of categorical (e.g., gender, education, etc) and numerical features (e.g., age, income, etc). You will predict the loan_status variable, which indicates whether a loan application was approved (1) or rejected (0).\n",
    "\n",
    "**On Data Pre-processing**: This dataset contains categorical featuers which are encoded as text. For our models to interpret these features, it is necessary to preprocess this dataset &mdash; in this case, convert categorical features into one-hot encoding &mdash; by using tools from pandas (https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). Example code is provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:57:26.477485Z",
     "start_time": "2025-03-23T14:57:14.845137Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sklearn.model_selection\n",
    "\n",
    "notebook_start_time = time.time()\n",
    "loan_data_full = pd.read_csv(\"loan_data.csv\")\n",
    "\n",
    "# Display a sample of the original dataset, which includes both categorical and numerical features\n",
    "loan_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary statistics of the dataset to check key attributes such as sample count, mean, and standard deviation of numerical features\n",
    "loan_data_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: Convert categorical features into one-hot encoded format\n",
    "# The categorical features being encoded include: person_gender, person_education, person_home_ownership, loan_intent, and previous_loan_defaults_on_file\n",
    "loan_data = pd.get_dummies(loan_data_full, columns=['person_home_ownership', 'loan_intent', 'person_education', 'previous_loan_defaults_on_file', 'person_gender'])\n",
    "# Remove redundant columns to avoid multicollinearity\n",
    "loan_data = loan_data.drop(columns=['person_gender_male', 'previous_loan_defaults_on_file_No'])\n",
    "\n",
    "# Display a sample of the dataset after preprocessing\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Model Training and Testing (4 marks)\n",
    "**On Model and Training Objective Function**: Train a binary linear classifier by minimising a hinge loss with L2 (ridge) regularisation. Specifically, given a set of $N$ training samples $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$, where $\\mathbf{x}_i$ is the feature vector and $y_i \\in \\{-1, +1\\}$ is the class label for the $i$-th training sample, the training objective function to minimise is\n",
    "$$O = C \\sum^N_{i=1}\\max\\left(0, 1 - y_i \\left(\\mathbf{w}^T\\mathbf{x}_i + w_0\\right)\\right) + \\frac{1}{2}\\mathbf{w}^T\\mathbf{w}. $$\n",
    "Here, $\\mathbf{w}$ is a column weight vector of the linear model, $w_0$ is the bias parameter of the model, and $C$ is the regularisation hyperparameter.\n",
    "\n",
    "**Instruction for Implementing `linear_gd_train`**: Complete the implementation of the training function `linear_gd_train` below, which trains a linear model by minimising the above training loss using gradient descent. The function should return the trained model weights and the corresponding objective function value per iteration. In addition to the training data, the function should take the regularisation hyperparameter $C$, learning rate $\\eta$, and the number of iterations $N_{max}$ as arguments. A reasonably good setting of these parameters has been provided below. *Marking notes: Scikit-learn and PyTorch are NOT allowed for implementating* `linear_gd_train`*. You should avoid using* `for` *loops in your implementation of the objective function or weight update, and instead use built-in numpy operations for efficiency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T22:19:27.850583Z",
     "start_time": "2025-03-19T22:19:27.842516Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_gd_train(data, labels, c=0.2, n_iters=200, learning_rate=0.0001, random_state=None # Add any other arguments here if needed\n",
    "          ):\n",
    "    \"\"\"\n",
    "    A summary of your function goes here.\n",
    "\n",
    "    data: training data\n",
    "    labels: training labels (boolean)\n",
    "    c: regularisation parameter\n",
    "    n_iters: number of iterations\n",
    "    learning_rate: learning rate for gradient descent\n",
    "\n",
    "    Returns an array of cost and model weights per iteration.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility if using random initialisation of weights (optional)\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "\n",
    "    # Create design matrix and labels\n",
    "    X_tilde = ...\n",
    "    y = ...\n",
    "\n",
    "    # Weight initialisation: use e.g. rng.standard_normal() or all zeros\n",
    "    w = ...\n",
    "\n",
    "    # Initialise arrays to store weights and cost at each iteration\n",
    "    w_all = ...\n",
    "    cost_all = ...\n",
    "    \n",
    "    # GD update of weights\n",
    "    for i in range(n_iters):\n",
    "        # Cost and gradient update of the linear model\n",
    "        cost = ...\n",
    "        \n",
    "        # Weight update\n",
    "        w = ...\n",
    "        \n",
    "        # save w and cost of each iteration in w_all and cost_all\n",
    "\n",
    "\n",
    "    # Return model parameters.\n",
    "    return cost_all, w_all\n",
    "\n",
    "\n",
    "def linear_predict(data, w):\n",
    "    \"\"\"\n",
    "    A summary of your function goes here.\n",
    "\n",
    "    data: test data\n",
    "    w: model weights\n",
    "\n",
    "    Returns the predicted labels.\n",
    "    \"\"\"\n",
    "\n",
    "    X_tilde = ...\n",
    "    y_pred = ...\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Data Splitting**: Use the provided code below to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:21:52.222663Z",
     "start_time": "2025-03-20T00:21:48.521789Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate the features and target variable\n",
    "binary_features = loan_data.drop(columns=['loan_status'])\n",
    "binary_targets = loan_data['loan_status']\n",
    "\n",
    "# Named _cls to keep our classification experiments distinct from regression\n",
    "train_X_cls, test_X_cls, train_y_cls, test_y_cls = sklearn.model_selection.train_test_split(binary_features, binary_targets, test_size=0.15, stratify=binary_targets)\n",
    "\n",
    "# Standardise the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X_cls = scaler.fit_transform(train_X_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction for Classification Experiment**: Write your code below to (1) train the model, (2) plot the training objective function value and the classification accuracy of the training set over iterations, and (3) print the classification accuracy and $F_1$ score of the testing set. Use the default setting provided in `linear_gd_train` for $C$, $\\eta$ and $N_{max}$. Your plot should have axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Plot accuracy and cost per iteration on training set\n",
    "\n",
    "# Standardise the test dataset\n",
    "\n",
    "# Predict on test set, report accuracy and f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Learning Rate Analysis (4 marks)\n",
    "The learning rate $\\eta$ (Greek letter \"eta\") is a key parameter that affects the model training and performance. Design an appropriate experiment to demonstrate the effect of $\\eta$ on model training, and on the model performance during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Report (6 Marks)\n",
    "1. Summarize (1) your process of constructing the loss function and computing its gradient used by `linear_gd_train`, (2) how your Python implementation minimizes this function, and (3) key insights learned from the task. (*3 marks*)\n",
    "2. Draw conclusions about your model behaviour and data from your plot produced in Section 1.2 based on classification accuracies of your training and testing sets? (*1 mark*)\n",
    "3. Discuss the effect of η on model training and on testing performance, based on your observations obtained in Section 1.3. (*2 marks*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Soybean Production Prediction by MLP (16 marks)\n",
    "###  2.1 Dataset and Experiment Preparation\n",
    "\n",
    "**On Dataset**: The provided \"Soybean Agricultural Dataset\" contains agricultural parameters related to soybean plant growth and production. It includes 52,678 instances with 13 features, covering plant characteristics, environmental conditions, and treatment factors, such as genotype, salicylic acid treatment, water stress, plant height, number of pods, chlorophyll content, etc. You will build a regression model by MLP to predict the soybean production, which reflects the crop yield under different experimental conditions.\n",
    "\n",
    "**On Data Pre-processing**: This dataset also needs pre-processing. Example code on conducting one-hot encoding to convert string features to numerical values is provided below. Further pre-processing techniques are provided by Scikit-Learn data pre-processing tools (https://scikit-learn.org/stable/modules/preprocessing.html), which you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Read data\n",
    "soybean_data_full = pd.read_csv(\"soybean_data.csv\")\n",
    "\n",
    "# Display a sample of the original dataset\n",
    "soybean_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T00:56:09.945889Z",
     "start_time": "2025-03-20T00:56:09.439763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get and display all unique values from the 'Parameters' column to identify distinct experimental conditions\n",
    "soybean_data_full['Parameters'].unique()\n",
    "\n",
    "# NOTE: The 'Parameters' column is a string encoding experimental conditions.\n",
    "# We need to extract numerical values from this string and create separate columns for each parameter.\n",
    "# Then, we will apply one-hot encoding to categorical variables to prepare the data for machine learning models.\n",
    "# G: Refers to the genotype of soybean, consisting of six different genotypes.\n",
    "# C: Represents salicylic acid, which has two levels (250 mg and 450 mg), along with a third level as a standard control.\n",
    "# S: Indicates water stress, which includes two levels:\n",
    "#       - Water stress at 5% of field capacity.\n",
    "#       - Water stress at 70% of field capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the individual parameters from the 'parameters' column\n",
    "soybean_data_processed = soybean_data_full.copy()\n",
    "# Extract the genotype and one-hot encode it\n",
    "encoder = OneHotEncoder()\n",
    "genotypes = encoder.fit_transform(soybean_data_full['Parameters'].str.extract(r'G(\\d)')).toarray()\n",
    "soybean_data_processed = pd.concat([soybean_data_processed, pd.DataFrame(genotypes, columns=[f'G{i}' for i in range(1, 7)])], axis=1)\n",
    "\n",
    "# Extract the salicylic acid treatment and encode it as 0, 250 mg, or 450 mg\n",
    "# 1 = 250 mg, 2 = 450 mg, 3 = control\n",
    "salicylic_acid = soybean_data_full['Parameters'].str.extract(r'C(\\d+)').astype(float)\n",
    "salicylic_acid = salicylic_acid.replace({1: 250, 2: 450, 3: 0})\n",
    "soybean_data_processed['Salicylic acid (mg)'] = salicylic_acid\n",
    "\n",
    "# Extract the water stress treatment and encode it as .05 or .7 of field capacity\n",
    "water_stress = soybean_data_full['Parameters'].str.extract(r'S(\\d)').astype(float)\n",
    "water_stress = water_stress.replace({1: .05, 2: .7})\n",
    "soybean_data_processed['Water Stress (pct field capacity)'] = water_stress\n",
    "\n",
    "# Drop the original 'Parameters' column as well as 'Random' column\n",
    "soybean_data_processed.drop(columns=['Parameters', 'Random '], inplace=True)\n",
    "\n",
    "# Display a sample of the dataset after preprocessing\n",
    "soybean_data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display summary statistics of the dataset to check key attributes such as sample count, mean, and standard deviation of numerical features\n",
    "soybean_data_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 MLP Model Selection (4 marks)\n",
    "This exercise focuses on the practical usage and implementation of MLPs. Key hyper-parameters that can affect the MLP performance include model architecture, activation function, and the number of training iterations.\n",
    "\n",
    "**Instruction on MLP Model Options**: You should experiment with the following model options.\n",
    "\n",
    "* *MLP architectures* including two single-hidden-layer MLPs, one with 3 hidden neurons (small) and another with 100 hidden neurons (large), and two two-hidden-layer MLPs, one with (3,3) neurons (small) and another with (100,100) neurons (large).\n",
    "* *Activation functions* including the logistic and ReLU activations.\n",
    "* *Numbers of training iterations* including three different iteration numbers.\n",
    "\n",
    "Model variations resulted from the above configurations are defined in the 'param_grid' below. All other hyperparameters follow Scikit-Learn’s default settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T01:18:41.823787Z",
     "start_time": "2025-03-20T01:18:41.774833Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'hidden_layer_sizes': [(3,), (100,), (3, 3), (100, 100)],\n",
    "        'activation': ['relu', 'logistic'],\n",
    "        'max_iter': [50, 200, 500]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T01:18:28.327352Z",
     "start_time": "2025-03-20T01:18:28.319970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define regression target and features, perform train-test split\n",
    "target_col = 'Seed Yield per Unit Area (SYUA)'\n",
    "regression_targets = soybean_data_processed[target_col].to_numpy()\n",
    "soybean_data = soybean_data_processed.drop(columns=[target_col])\n",
    "regression_data = soybean_data.to_numpy()\n",
    "train_X_regr, test_X_regr, train_y_regr, test_y_regr = sklearn.model_selection.train_test_split(regression_data, regression_targets, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction for Regression Experiment**: Write your code below to (1) preprocess the data by standardisation (or any other pre-processing technique that you see fit), and (2) perform model selection, train and test your MLP regressors. Use the provided training set for model selection by cross-validation, and use mean squared error (MSE) as the model selection performance metric. You can use the scikit-learn module [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) to conduct grid search. Print the cross-validation MSE with standard deviation for the selected model. Re-train the selected model using the whole training set, and print its MSE and $R^2$ score for the testing set.\n",
    "\n",
    "**Marking Note:** This section can often take a long time to run when a large number of models are being trained. If you are concerned about the runtime when submitting your notebook, please copy the output of the entire grid search into a markdown cell so that we can see the results. Then, re-define the param_grid so that only two models are trained. This will allow us to see that your code works without having to wait for the entire grid search to complete during marking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Pre-process your data\n",
    "\n",
    "\n",
    "# Define MLP model\n",
    "\n",
    "\n",
    "# Initialise and fit the grid search\n",
    "\n",
    "\n",
    "# Report the best parameters and the CV results\n",
    "\n",
    "\n",
    "# Report model performance with best parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Importance Testing (4 Marks)\n",
    "\n",
    "In real-world regression application, the accuracy of the predicted output depends on multiple input features, but not all features contribute equally. Often, some features play a significant role, while some have a minor impact on the prediction. It is useful to identify feature importance for a prediction task. Gradient Boosting is a technique for such purpose, based on which scikit-learn provides a tool [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html). Using this tool, we provide you the function `feature_importance_score_cal` to generate feature importance scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: generate importance scores using Gradient Boosting Methods\n",
    "# Note: parameters have not been tuned\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def feature_importance_score_cal(train_X, train_y, test_X, test_y, feature_names):\n",
    "    #### Train the GradientBoostingRegressor\n",
    "    params = {\n",
    "        'n_estimators': 400,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.7,\n",
    "    }\n",
    "    gbr = GradientBoostingRegressor(**params, random_state=42)\n",
    "    # fit on training data, apply to test data\n",
    "    gbr.fit(train_X, train_y)\n",
    "    test_mse = mean_squared_error(test_y, gbr.predict(test_X))\n",
    "    test_r2 = gbr.score(test_X, test_y)\n",
    "    print(f\"Gradient boosting regressor on full test set gives MSE: {test_mse:.4f} and R^2 score: {test_r2:.4f}\")\n",
    "    feature_importance_score = gbr.feature_importances_\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importance_score)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(pos, feature_importance_score[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "    plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "    ## Box plot showing the variance of feature importance scores\n",
    "    result = permutation_importance(\n",
    "        gbr, test_X, test_y, n_repeats=10, random_state=42, n_jobs=2\n",
    "    )\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(\n",
    "        result.importances[sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=np.array(feature_names)[sorted_idx], # newer versions of matplotlib may use tick_labels as kwarg instead\n",
    "    )\n",
    "    plt.title(\"Permutation Importance (test set)\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return feature_importance_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction on Experiment**: Design an experiment and write your code below to (1) select important features using `feature_importance_score_cal` and (2) validate the importance of the features in the dataset. **Hint**: If five features are identified as significantly more important than the others, will a good prediction accuracy be obtained by using only these five features, e.g., no significant accuracy drop as compared to using the full feature set? You may wish to read further on [feature selection](https://scikit-learn.org/stable/modules/feature_selection.html) and [Permutation Importance vs Random Forest Feature Importance (MDI)](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html) before approaching this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Calculate feature importance scores using feature_importance_score_cal()\n",
    "feature_names = soybean_data_processed.drop(columns=[target_col]).columns.values\n",
    "\n",
    "# Select important features\n",
    "\n",
    "# Validate the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 External Testing (3 Marks)\n",
    "Develop a robust scikit-learn MLP model for soybean production prediction, and submit it along with your notebook and report. It will be run and evaluated on a test set containing soybean instances unseen by you. **Please note that the unseen dataset may contain noisy or missing features. Your model should be able to handle such cases.** Hint: you may want to experiment with model hyperparameters and data processing. You may find the sklearn module [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) useful when saving your completed model.\n",
    "\n",
    "**Important: set your university username (e.g. mbxxabc3) below when saving your model.** Failure to do this correctly would lead to your model not being marked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_eval_utils\n",
    "\n",
    "#### SAVE YOUR MODEL\n",
    "student_username = \"mbxxxxx0\" # SET YOUR USERNAME HERE\n",
    "model = ... # SET YOUR MODEL HERE\n",
    "model_eval_utils.save_model(student_username, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total notebook run time: {time.time() - notebook_start_time:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to test your saved model\n",
    "Use the `run_model()` function to make sure your saved model can be loaded and run before submitting.\n",
    "\n",
    "**Disclaimers:** Please note the score returned by `run_model()` is not in any way indicative of your final mark. This is just a simple test to make sure your model can be loaded and run, though there is no guarantee that your model will run on the unseen data just because it can be run here. When testing your model, the GTA will run your model following the practice below, but replacing the bunk_data with the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some random data from the training set\n",
    "bunk_sample_indices = np.random.choice(train_X_regr.shape[0], size=3, replace=False)\n",
    "# Extract the selected rows\n",
    "bunk_sampled_X = train_X_regr[bunk_sample_indices]\n",
    "bunk_sampled_y = train_y_regr[bunk_sample_indices]\n",
    "\n",
    "score = model_eval_utils.run_model(student_username,\n",
    "                                test_data=bunk_sampled_X,\n",
    "                                test_labels=bunk_sampled_y,\n",
    "                                model_folder=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Report (5 Marks)\n",
    "1. Draw conclusions from your model selection results obtained in Section 2.2, based on factors like prediction accuracy, training efficiency, and model complexity. (*2 marks*)\n",
    "2. Describe the design process of your selection and validation methods used in Section 2.3, and discuss your observations from your validation experiment. (*2 marks*)\n",
    "3. Describe the methods you have used to address missing and noisy features in Section 2.4. (*1 mark*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
